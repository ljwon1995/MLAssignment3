{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = './horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1027, shuffle=False, num_workers=1)  \n",
    "\n",
    "validation_data_path = './horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=1027, shuffle=False, num_workers=1)  \n",
    "\n",
    "\n",
    "trainList = list()\n",
    "validList = list()\n",
    "trainLabelList = list()\n",
    "validLabelList = list()\n",
    "\n",
    "for i, data in enumerate(trainloader):\n",
    "    # inputs is the image\n",
    "    # labels is the class of the image\n",
    "    inputs, labels = data\n",
    "\n",
    "\n",
    "    # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "\n",
    "    batch_size = inputs.shape[0]\n",
    "\n",
    "    # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "    # change inputs to matrix 10000*batch_size\n",
    "    for bat_idx in range(batch_size):\n",
    "\n",
    "        targMat = inputs[bat_idx][0]\n",
    "\n",
    "        colVec = np.reshape(targMat, (np.product(targMat.shape), 1), 'F')\n",
    "\n",
    "        if(bat_idx == 0):\n",
    "            batMat = colVec\n",
    "        else:\n",
    "            batMat = np.concatenate((batMat, colVec), axis = 1)         \n",
    "\n",
    "    # Add ones because of the value b in coefficient\n",
    "    ones = np.ones((1, batch_size), dtype = int)\n",
    "    batMat = np.concatenate((batMat, ones))\n",
    "    trainList.append(batMat)\n",
    "    trainLabelList.append(labels)\n",
    "    \n",
    "# load validation images of the batch size for every iteration\n",
    "for i, data in enumerate(valloader):\n",
    "\n",
    "    # inputs is the image\n",
    "    # labels is the class of the image\n",
    "    inputs, labels = data\n",
    "\n",
    "    # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "     # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "\n",
    "    batch_size = inputs.shape[0]\n",
    "\n",
    "    # Change Inputs to matrix 10000*batch_size\n",
    "\n",
    "    for bat_idx in range(batch_size):\n",
    "        targMat = inputs[bat_idx][0]\n",
    "        colVec = np.reshape(targMat, (np.product(targMat.shape), 1), 'F')\n",
    "\n",
    "        if(bat_idx == 0):\n",
    "            batMat = colVec\n",
    "        else:\n",
    "            batMat = np.concatenate((batMat,colVec), axis = 1)\n",
    "        \n",
    "    # Add ones because of the value b in coefficient\n",
    "    ones = np.ones((1, batch_size), dtype = int)\n",
    "    batMat = np.concatenate((batMat, ones))\n",
    "    validList.append(batMat)\n",
    "    validLabelList.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1027)\n",
      "epoch :  0 , lrnCor :  500 valCor :  128 elapsed time :  0.06604599952697754\n",
      "(3, 1027)\n",
      "epoch :  1 , lrnCor :  527 valCor :  128 elapsed time :  0.043910980224609375\n",
      "(3, 1027)\n",
      "epoch :  2 , lrnCor :  527 valCor :  128 elapsed time :  0.04123497009277344\n",
      "(3, 1027)\n",
      "epoch :  3 , lrnCor :  527 valCor :  128 elapsed time :  0.04424619674682617\n",
      "(3, 1027)\n",
      "epoch :  4 , lrnCor :  527 valCor :  128 elapsed time :  0.043254852294921875\n",
      "(3, 1027)\n",
      "epoch :  5 , lrnCor :  527 valCor :  128 elapsed time :  0.04179096221923828\n",
      "(3, 1027)\n",
      "epoch :  6 , lrnCor :  527 valCor :  128 elapsed time :  0.04171919822692871\n",
      "(3, 1027)\n",
      "epoch :  7 , lrnCor :  527 valCor :  128 elapsed time :  0.04902291297912598\n",
      "(3, 1027)\n",
      "epoch :  8 , lrnCor :  527 valCor :  128 elapsed time :  0.04833579063415527\n",
      "(3, 1027)\n",
      "epoch :  9 , lrnCor :  527 valCor :  128 elapsed time :  0.045929908752441406\n",
      "(3, 1027)\n",
      "epoch :  10 , lrnCor :  527 valCor :  128 elapsed time :  0.054574012756347656\n",
      "(3, 1027)\n",
      "epoch :  11 , lrnCor :  527 valCor :  128 elapsed time :  0.05210518836975098\n",
      "(3, 1027)\n",
      "epoch :  12 , lrnCor :  527 valCor :  128 elapsed time :  0.04615306854248047\n",
      "(3, 1027)\n",
      "epoch :  13 , lrnCor :  527 valCor :  128 elapsed time :  0.044261932373046875\n",
      "(3, 1027)\n",
      "epoch :  14 , lrnCor :  527 valCor :  128 elapsed time :  0.04328489303588867\n",
      "(3, 1027)\n",
      "epoch :  15 , lrnCor :  527 valCor :  128 elapsed time :  0.04807925224304199\n",
      "(3, 1027)\n",
      "epoch :  16 , lrnCor :  527 valCor :  128 elapsed time :  0.06058907508850098\n",
      "(3, 1027)\n",
      "epoch :  17 , lrnCor :  527 valCor :  128 elapsed time :  0.08380007743835449\n",
      "(3, 1027)\n",
      "epoch :  18 , lrnCor :  527 valCor :  128 elapsed time :  0.06725192070007324\n",
      "(3, 1027)\n",
      "epoch :  19 , lrnCor :  527 valCor :  128 elapsed time :  0.05251193046569824\n",
      "(3, 1027)\n",
      "epoch :  20 , lrnCor :  527 valCor :  128 elapsed time :  0.06989002227783203\n",
      "(3, 1027)\n",
      "epoch :  21 , lrnCor :  527 valCor :  128 elapsed time :  0.05743288993835449\n",
      "(3, 1027)\n",
      "epoch :  22 , lrnCor :  527 valCor :  128 elapsed time :  0.054017066955566406\n",
      "(3, 1027)\n",
      "epoch :  23 , lrnCor :  527 valCor :  128 elapsed time :  0.05486321449279785\n",
      "(3, 1027)\n",
      "epoch :  24 , lrnCor :  527 valCor :  128 elapsed time :  0.07881617546081543\n",
      "(3, 1027)\n",
      "epoch :  25 , lrnCor :  527 valCor :  128 elapsed time :  0.05182623863220215\n",
      "(3, 1027)\n",
      "epoch :  26 , lrnCor :  527 valCor :  128 elapsed time :  0.06067085266113281\n",
      "(3, 1027)\n",
      "epoch :  27 , lrnCor :  527 valCor :  128 elapsed time :  0.0495600700378418\n",
      "(3, 1027)\n",
      "epoch :  28 , lrnCor :  527 valCor :  128 elapsed time :  0.042797088623046875\n",
      "(3, 1027)\n",
      "epoch :  29 , lrnCor :  527 valCor :  128 elapsed time :  0.05368804931640625\n",
      "(3, 1027)\n",
      "epoch :  30 , lrnCor :  527 valCor :  128 elapsed time :  0.0644388198852539\n",
      "(3, 1027)\n",
      "epoch :  31 , lrnCor :  527 valCor :  128 elapsed time :  0.06006813049316406\n",
      "(3, 1027)\n",
      "epoch :  32 , lrnCor :  527 valCor :  128 elapsed time :  0.05763602256774902\n",
      "(3, 1027)\n",
      "epoch :  33 , lrnCor :  527 valCor :  128 elapsed time :  0.05440783500671387\n",
      "(3, 1027)\n",
      "epoch :  34 , lrnCor :  527 valCor :  128 elapsed time :  0.05287504196166992\n",
      "(3, 1027)\n",
      "epoch :  35 , lrnCor :  527 valCor :  128 elapsed time :  0.0574951171875\n",
      "(3, 1027)\n",
      "epoch :  36 , lrnCor :  527 valCor :  128 elapsed time :  0.05996584892272949\n",
      "(3, 1027)\n",
      "epoch :  37 , lrnCor :  527 valCor :  128 elapsed time :  0.0576019287109375\n",
      "(3, 1027)\n",
      "epoch :  38 , lrnCor :  527 valCor :  128 elapsed time :  0.05703902244567871\n",
      "(3, 1027)\n",
      "epoch :  39 , lrnCor :  527 valCor :  128 elapsed time :  0.05614209175109863\n",
      "(3, 1027)\n",
      "epoch :  40 , lrnCor :  527 valCor :  128 elapsed time :  0.059516191482543945\n",
      "(3, 1027)\n",
      "epoch :  41 , lrnCor :  527 valCor :  128 elapsed time :  0.05675077438354492\n",
      "(3, 1027)\n",
      "epoch :  42 , lrnCor :  527 valCor :  128 elapsed time :  0.061808109283447266\n",
      "(3, 1027)\n",
      "epoch :  43 , lrnCor :  527 valCor :  128 elapsed time :  0.05060291290283203\n",
      "(3, 1027)\n",
      "epoch :  44 , lrnCor :  527 valCor :  128 elapsed time :  0.04074907302856445\n",
      "(3, 1027)\n",
      "epoch :  45 , lrnCor :  527 valCor :  128 elapsed time :  0.044100046157836914\n",
      "(3, 1027)\n",
      "epoch :  46 , lrnCor :  527 valCor :  128 elapsed time :  0.04144692420959473\n",
      "(3, 1027)\n",
      "epoch :  47 , lrnCor :  527 valCor :  128 elapsed time :  0.040642738342285156\n",
      "(3, 1027)\n",
      "epoch :  48 , lrnCor :  527 valCor :  128 elapsed time :  0.04131889343261719\n",
      "(3, 1027)\n",
      "epoch :  49 , lrnCor :  527 valCor :  128 elapsed time :  0.0405881404876709\n",
      "(3, 1027)\n",
      "epoch :  50 , lrnCor :  527 valCor :  128 elapsed time :  0.04593777656555176\n",
      "(3, 1027)\n",
      "epoch :  51 , lrnCor :  527 valCor :  128 elapsed time :  0.04064202308654785\n",
      "(3, 1027)\n",
      "epoch :  52 , lrnCor :  527 valCor :  128 elapsed time :  0.04067206382751465\n",
      "(3, 1027)\n",
      "epoch :  53 , lrnCor :  527 valCor :  128 elapsed time :  0.050769805908203125\n",
      "(3, 1027)\n",
      "epoch :  54 , lrnCor :  527 valCor :  128 elapsed time :  0.04639005661010742\n",
      "(3, 1027)\n",
      "epoch :  55 , lrnCor :  527 valCor :  128 elapsed time :  0.044936180114746094\n",
      "(3, 1027)\n",
      "epoch :  56 , lrnCor :  527 valCor :  128 elapsed time :  0.04594302177429199\n",
      "(3, 1027)\n",
      "epoch :  57 , lrnCor :  527 valCor :  128 elapsed time :  0.04979395866394043\n",
      "(3, 1027)\n",
      "epoch :  58 , lrnCor :  527 valCor :  128 elapsed time :  0.04061484336853027\n",
      "(3, 1027)\n",
      "epoch :  59 , lrnCor :  527 valCor :  128 elapsed time :  0.040689945220947266\n",
      "(3, 1027)\n",
      "epoch :  60 , lrnCor :  527 valCor :  128 elapsed time :  0.04014897346496582\n",
      "(3, 1027)\n",
      "epoch :  61 , lrnCor :  527 valCor :  128 elapsed time :  0.040225982666015625\n",
      "(3, 1027)\n",
      "epoch :  62 , lrnCor :  527 valCor :  128 elapsed time :  0.04647517204284668\n",
      "(3, 1027)\n",
      "epoch :  63 , lrnCor :  527 valCor :  128 elapsed time :  0.04080986976623535\n",
      "(3, 1027)\n",
      "epoch :  64 , lrnCor :  527 valCor :  128 elapsed time :  0.0413360595703125\n",
      "(3, 1027)\n",
      "epoch :  65 , lrnCor :  527 valCor :  128 elapsed time :  0.043782949447631836\n",
      "(3, 1027)\n",
      "epoch :  66 , lrnCor :  527 valCor :  128 elapsed time :  0.04416394233703613\n",
      "(3, 1027)\n",
      "epoch :  67 , lrnCor :  527 valCor :  128 elapsed time :  0.04021596908569336\n",
      "(3, 1027)\n",
      "epoch :  68 , lrnCor :  527 valCor :  128 elapsed time :  0.03943610191345215\n",
      "(3, 1027)\n",
      "epoch :  69 , lrnCor :  527 valCor :  128 elapsed time :  0.04027700424194336\n",
      "(3, 1027)\n",
      "epoch :  70 , lrnCor :  527 valCor :  128 elapsed time :  0.0459141731262207\n",
      "(3, 1027)\n",
      "epoch :  71 , lrnCor :  527 valCor :  128 elapsed time :  0.04128003120422363\n",
      "(3, 1027)\n",
      "epoch :  72 , lrnCor :  527 valCor :  128 elapsed time :  0.04087209701538086\n",
      "(3, 1027)\n",
      "epoch :  73 , lrnCor :  527 valCor :  128 elapsed time :  0.040403127670288086\n",
      "(3, 1027)\n",
      "epoch :  74 , lrnCor :  527 valCor :  128 elapsed time :  0.044522762298583984\n",
      "(3, 1027)\n",
      "epoch :  75 , lrnCor :  527 valCor :  128 elapsed time :  0.04032301902770996\n",
      "(3, 1027)\n",
      "epoch :  76 , lrnCor :  527 valCor :  128 elapsed time :  0.040377140045166016\n",
      "(3, 1027)\n",
      "epoch :  77 , lrnCor :  527 valCor :  128 elapsed time :  0.04376411437988281\n",
      "(3, 1027)\n",
      "epoch :  78 , lrnCor :  527 valCor :  128 elapsed time :  0.04695773124694824\n",
      "(3, 1027)\n",
      "epoch :  79 , lrnCor :  527 valCor :  128 elapsed time :  0.04634284973144531\n",
      "(3, 1027)\n",
      "epoch :  80 , lrnCor :  527 valCor :  128 elapsed time :  0.04789113998413086\n",
      "(3, 1027)\n",
      "epoch :  81 , lrnCor :  527 valCor :  128 elapsed time :  0.05874490737915039\n",
      "(3, 1027)\n",
      "epoch :  82 , lrnCor :  527 valCor :  128 elapsed time :  0.0453648567199707\n",
      "(3, 1027)\n",
      "epoch :  83 , lrnCor :  527 valCor :  128 elapsed time :  0.0455479621887207\n",
      "(3, 1027)\n",
      "epoch :  84 , lrnCor :  527 valCor :  128 elapsed time :  0.04604601860046387\n",
      "(3, 1027)\n",
      "epoch :  85 , lrnCor :  527 valCor :  128 elapsed time :  0.0556337833404541\n",
      "(3, 1027)\n",
      "epoch :  86 , lrnCor :  527 valCor :  128 elapsed time :  0.04193305969238281\n",
      "(3, 1027)\n",
      "epoch :  87 , lrnCor :  527 valCor :  128 elapsed time :  0.04078197479248047\n",
      "(3, 1027)\n",
      "epoch :  88 , lrnCor :  527 valCor :  128 elapsed time :  0.04109907150268555\n",
      "(3, 1027)\n",
      "epoch :  89 , lrnCor :  527 valCor :  128 elapsed time :  0.042513132095336914\n",
      "(3, 1027)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-bfdc1052f9cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mcor\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatIdx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtrainLabelList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatIdx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mcor\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Initialize Coef to Zeros\n",
    "w_0 = np.zeros((4,10001), dtype = float)\n",
    "w_1 = np.zeros((3, 4), dtype = float)\n",
    "w_2 = np.zeros((1, 3), dtype = float)\n",
    "\n",
    "#Set Learning Rate\n",
    "lrnRate = 0.002\n",
    "\n",
    "# Set Loss Lists\n",
    "lrnLoss = list()\n",
    "valLoss = list()\n",
    "\n",
    "# Set Accurate Lists\n",
    "lrnAcc = list()\n",
    "valAcc = list()\n",
    "\n",
    "# set Elapsed time Lists\n",
    "elapTime = list()\n",
    "\n",
    "\n",
    "epoch = -1\n",
    "lrnAccRate = 0\n",
    "\n",
    "while(lrnAccRate < 0.85):\n",
    "    epoch += 1\n",
    "    # load training images of the batch size for every iteration\n",
    "    \n",
    "    #Set Sum of Derivatives to 0\n",
    "    sumDw_0 = np.zeros((10001,4), dtype = float)\n",
    "    \n",
    "    #Set Sum of Loss to 0\n",
    "    sumL = 0\n",
    "    \n",
    "    #Set Sum of Cor to 0\n",
    "    cor = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, a_0 in enumerate(trainList):\n",
    "        \n",
    "        batch_size = a_0.shape[1]\n",
    "\n",
    "\n",
    "        # Start Regression Calculation\n",
    "        z_0 = np.dot(w_0, a_0)                                          \n",
    "        a_1 = 1/(1 + np.exp(-z_0))                                       \n",
    "        \n",
    "        z_1 = np.dot(w_1, a_1)\n",
    "        a_2 = 1/(1 + np.exp(-z_1))\n",
    "        \n",
    "        z_2 = np.dot(w_2, a_2)\n",
    "        a_3 = 1/(1 + np.exp(-z_2))\n",
    "        \n",
    "        \n",
    "    \n",
    "        dz_2 = np.subtract(a_3, trainLabelList[i])  \n",
    "        dw_2 = np.dot(dz_2, a_2.T)\n",
    "        w_2 -= lrnRate * dw_2\n",
    "        \n",
    "        da_2 = np.dot(w_2.T, dz_2)\n",
    "        dz_1 = da_2 * a_2 * (1 - a_2) \n",
    "        dw_1 = np.dot(dz_1, a_1.T)\n",
    "        w_1 -= lrnRate * dw_1\n",
    "        \n",
    "        da_1 = np.dot(w_1.T, dz_1)\n",
    "        dz_0 = da_1 * a_1 * (1 - a_1)\n",
    "        dw_0 = np.dot(dz_0, a_0.T)\n",
    "        w_0 -= lrnRate * dw_0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    "        \n",
    "        # Calculate Total Loss\n",
    "        a_3 = torch.from_numpy(a_3)                                             #change ndarray to tensor\n",
    "        dLabels = trainLabelList[i].double()                                   #change tensor type to double\n",
    "        L = -(dLabels) * np.log(a_3) - (1-dLabels) * np.log(1-a_3)      \n",
    "        sumL += L.sum()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        \n",
    "\n",
    "        for batIdx in range(batch_size):\n",
    "\n",
    "            if(a_3[0][batIdx] <= 0.5 and trainLabelList[i][batIdx] == 0):\n",
    "                cor += 1\n",
    "\n",
    "            if(a_3[0][batIdx] > 0.5 and trainLabelList[i][batIdx] == 1):\n",
    "                cor += 1\n",
    "            \n",
    "        \n",
    "    \n",
    "    totalDataNum = len(trainloader.dataset)\n",
    "        \n",
    "    # Calculate dLossdCoef\n",
    "    \n",
    "    \n",
    "\n",
    "    # Update coefs using derivatives\n",
    "    \n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapTime.append(elapsed_time)\n",
    "\n",
    "\n",
    "    # Calculate TotalLoss\n",
    "    sumL /= totalDataNum\n",
    "\n",
    "    lrnLoss.append(sumL)\n",
    "    \n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    \n",
    "    lrnAccRate = cor/totalDataNum\n",
    "    lrnAcc.append(lrnAccRate)\n",
    "    \n",
    "                \n",
    "\n",
    "\n",
    "    # Set Sum Of Valid Loss to 0\n",
    "    sumVL = 0\n",
    "    # Set Sum of Valid Cor to 0\n",
    "    vCor = 0\n",
    "    \n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, a_0 in enumerate(validList):\n",
    "        \n",
    "        \n",
    "        batch_size = batMat.shape[1]\n",
    "        \n",
    "        # Start Calculate Loss \n",
    "        z_0 = np.dot(w_0, a_0)\n",
    "        a_1= 1/(1+np.exp(-z_0))\n",
    "        \n",
    "        z_1 = np.dot(w_1, a_1)\n",
    "        a_2= 1/(1+np.exp(-z_1))\n",
    "        \n",
    "        z_2 = np.dot(w_2, a_2)\n",
    "        a_3= 1/(1+np.exp(-z_2))\n",
    "        \n",
    "        \n",
    "        a_3 = torch.from_numpy(a_3)\n",
    "        dLabels = validLabelList[i].double()\n",
    "        L = -(dLabels) * np.log(a_3) - (1-dLabels) * np.log(1-a_3)\n",
    "        sumVL += L.sum()\n",
    "\n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        \n",
    "        for batIdx in range(batch_size):\n",
    "            if(a_3[0][batIdx] <= 0.5 and validLabelList[i][batIdx] == 0):\n",
    "                vCor += 1\n",
    "                \n",
    "            if(a_3[0][batIdx] > 0.5 and validLabelList[i][batIdx] == 1):\n",
    "                vCor += 1\n",
    "    \n",
    "    totalValDataNum = len(valloader.dataset)\n",
    "    \n",
    "    # CalCulate Total Loss\n",
    "    sumVL /= totalValDataNum\n",
    "    valLoss.append(sumVL)\n",
    "    \n",
    "    \n",
    "    # Calculate Accuracy\n",
    "    vAcc = vCor/totalValDataNum\n",
    "    valAcc.append(vAcc)\n",
    "    \n",
    "    \n",
    "    print(\"epoch : \",epoch,',', \"lrnCor : \", cor, \"valCor : \", vCor, \"elapsed time : \", elapsed_time)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elapTime, color = 'black', label = \"ElapsedTime\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Loss\n",
    "\n",
    "fig, axs = plt.subplots(2,1, figsize = (10,4))\n",
    "\n",
    "\n",
    "axs[0].plot(lrnLoss, color = 'red', label = \"TrainingLoss\")\n",
    "\n",
    "\n",
    "axs[1].plot(valLoss, color = 'blue', label = \"ValidationLoss\")\n",
    "\n",
    "axs[0].set(ylabel = 'Loss')\n",
    "axs[1].set(xlabel = 'Iteration', ylabel = 'Loss')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Accuracy\n",
    "fig, axs = plt.subplots(2,1, figsize = (10,5))\n",
    "\n",
    "\n",
    "axs[0].plot(lrnAcc, color = 'orange', label = \"TrainingAccuracy\")\n",
    "\n",
    "\n",
    "axs[1].plot(valAcc, color = 'green', label = \"ValidationAccuracy\")\n",
    "\n",
    "axs[0].set(ylabel = 'Accuracy')\n",
    "axs[1].set(xlabel = 'Iteration', ylabel = 'Accuracy')\n",
    "\n",
    "axs[0].legend()\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Altogether\n",
    "\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.title('Altogether')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.plot(lrnLoss, color = 'red', label = 'TrainingLoss')\n",
    "plt.plot(valLoss, color = 'blue', label = \"ValidationLoss\")\n",
    "plt.plot(lrnAcc, color = 'orange', label = \"TrainingAccuracy\")\n",
    "plt.plot(valAcc, color = 'green', label = \"ValidationAccuracy\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  Dataset   |   Loss   | Accuracy\")\n",
    "\n",
    "print(\"  Training  | %.4f | %.4f\" % (lrnLoss[-1], lrnAcc[-1]))\n",
    "print(\" Validation| %.4f | %.4f\" % (valLoss[-1], valAcc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[ 0 -1 -2]\n",
      " [-3 -4 -5]\n",
      " [-6 -7 -8]]\n",
      "[[  0  -2  -6]\n",
      " [-12 -20 -30]\n",
      " [-42 -56 -72]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3],\n",
    "            [4,5,6],\n",
    "            [7,8,9]])\n",
    "print(a)\n",
    "\n",
    "print(1-a)\n",
    "print(a * (1-a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
